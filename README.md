# vllm_musa

> ä¸»è¦æ›´æ–°ï¼š
> - éå¸¸æ„Ÿè°¢è¸©å‘æ–‡ç« [ä¸­å›½ç§‘å­¦é™¢å¤§å­¦GPUæ¶æ„ä¸ç¼–ç¨‹å¤§ä½œä¸šäºŒ æ‘©å°”çº¿ç¨‹èµ›é“  (MTT S4000) AUTODLéƒ¨ç½²ä¸æµ‹è¯•æŒ‡å— - æ±‚ç´¢è€…freedomçš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1989072964000453732)ï¼Œä¿®å¤äº†æåˆ°çš„åŸä»“åº“ä¸­çš„è®¸å¤šbug
> - æ„å»ºäº†é€‚é…AutoDLæ‘©å°”çº¿ç¨‹çš„vLLMè½®å­


æ‘©å°”çº¿ç¨‹è‡´åŠ›äºæ„å»ºå®Œå–„å¥½ç”¨çš„å›½äº§GPUåº”ç”¨ç”Ÿæ€ï¼Œè‡ªä¸»ç ”å‘äº†MUSAæ¶æ„åŠè½¯ä»¶å¹³å°ã€‚vllmé¡¹ç›®æ˜¯ä¸šç•Œå¹¿æ³›ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å’ŒæœåŠ¡å¼•æ“ï¼Œä½¿ç”¨CUDA/ROCmæä¾›GPUåŠ é€Ÿèƒ½åŠ›ã€‚ä¸ºäº†æ–¹ä¾¿æ‘©å°”çº¿ç¨‹GPUç”¨æˆ·ä½¿ç”¨vllmæ¡†æ¶ï¼Œæˆ‘ä»¬å‘èµ·vllm_musaå¼€æºé¡¹ç›®ä¸ºvllmæä¾›MUSAåŠ é€Ÿï¼Œè®©ç”¨æˆ·å¯é‡Šæ”¾æ‘©å°”çº¿ç¨‹GPUçš„æ¾æ¹ƒç®—åŠ›ã€‚

ç°æœ‰çš„vllmä»£ç ä¸æ”¯æŒæ‘©å°”çº¿ç¨‹GPUä½œä¸ºåç«¯ï¼Œå› æ­¤æˆ‘ä»¬æ–°å¢äº†MUSAè®¾å¤‡åç«¯ã€‚vllm_musaæ¥å£ä¸å®˜æ–¹æ¥å£ä¸€è‡´ï¼Œç”¨æˆ·æ— éœ€æ”¹åŠ¨ä¸šåŠ¡ä»£ç ï¼Œå¼€ç®±å³ç”¨ã€‚

MUSAçš„ä¸€å¤§ä¼˜åŠ¿æ˜¯CUDAå…¼å®¹ï¼Œé€šè¿‡musifyå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿå°†å®˜æ–¹ä»£ç portingè‡³MUSAè½¯ä»¶æ ˆï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®æ–‡æ¡£è‡ªè¡Œå‡çº§vllmç‰ˆæœ¬å¹¶é€‚é…MUSAè½¯ä»¶æ ˆã€‚

## æ”¯æŒæ¨¡å‹åˆ—è¡¨âš ï¸
æ­¤ç‰ˆvllmæœ€åæ›´æ–°æ—¶é—´ä¸º2024å¹´å››æœˆï¼Œè¯·æ³¨æ„ä¸æ”¯æŒæœ€æ–°ç‰ˆçš„DeepSeekå’ŒQwen3ç­‰ç³»åˆ—æ¨¡å‹ï¼Œè¯¦ç»†æ”¯æŒåˆ—è¡¨å¦‚ä¸‹ï¼š

| Architecture | Models | Example HuggingFace Models | LoRA |
|-------------|--------|----------------------------|------|
| `AquilaForCausalLM` | Aquila | `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc. | âœ… |
| `BaiChuanForCausalLM` | Baichuan | `baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc. | âœ… |
| `ChatGLMModel` | ChatGLM | `THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc. | âœ… |
| `CohereForCausalLM` | Command-R | `CohereForAI/c4ai-command-r-v01`, etc. |  |
| `DbrxForCausalLM` | DBRX | `databricks/dbrx-base`, `databricks/dbrx-instruct`, etc. |  |
| `DeciLMForCausalLM` | DeciLM | `Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc. |  |
| `BloomForCausalLM` | BLOOM, BLOOMZ, BLOOMChat | `bigscience/bloom`, `bigscience/bloomz`, etc. |  |
| `FalconForCausalLM` | Falcon | `tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc. |  |
| `GemmaForCausalLM` | Gemma | `google/gemma-2b`, `google/gemma-7b`, etc. | âœ… |
| `GPT2LMHeadModel` | GPT-2 | `gpt2`, `gpt2-xl`, etc. |  |
| `GPTBigCodeForCausalLM` | StarCoder, SantaCoder, WizardCoder | `bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, `WizardLM/WizardCoder-15B-V1.0`, etc. |  |
| `GPTJForCausalLM` | GPT-J | `EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc. |  |
| `GPTNeoXForCausalLM` | GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM | `EleutherAI/gpt-neox-20b`, `EleutherAI/pythia-12b`, `OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc. |  |
| `InternLMForCausalLM` | InternLM | `internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc. | âœ… |
| `InternLM2ForCausalLM` | InternLM2 | `internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc. |  |
| `JAISLMHeadModel` | Jais | `core42/jais-13b`, `core42/jais-13b-chat`, `core42/jais-30b-v3`, `core42/jais-30b-chat-v3`, etc. |  |
| `LlamaForCausalLM` | LLaMA, Llama 2, Meta Llama 3, Vicuna, Alpaca, Yi | `meta-llama/Meta-Llama-3-8B-Instruct`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-13b-hf`, `meta-llama/Llama-2-70b-hf`, `openlm-research/open_llama_13b`, `lmsys/vicuna-13b-v1.3`, `01-ai/Yi-6B`, `01-ai/Yi-34B`, etc. | âœ… |
| `MiniCPMForCausalLM` | MiniCPM | `openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, etc. |  |
| `MistralForCausalLM` | Mistral, Mistral-Instruct | `mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc. | âœ… |
| `MixtralForCausalLM` | Mixtral-8x7B, Mixtral-8x7B-Instruct | `mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc. | âœ… |
| `MPTForCausalLM` | MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter | `mosaicml/mpt-7b`, `mosaicml/mpt-7b-storywriter`, `mosaicml/mpt-30b`, etc. |  |
| `OLMoForCausalLM` | OLMo | `allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc. |  |
| `OPTForCausalLM` | OPT, OPT-IML | `facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc. |  |
| `OrionForCausalLM` | Orion | `OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc. |  |
| `PhiForCausalLM` | Phi | `microsoft/phi-1_5`, `microsoft/phi-2`, etc. |  |
| `Phi3ForCausalLM` | Phi-3 | `microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, etc. |  |
| `QWenLMHeadModel` | Qwen | `Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc. |  |
| `Qwen2ForCausalLM` | Qwen2/Qwen2.5 | `Qwen/Qwen2-beta-7B`, `Qwen/Qwen2-beta-7B-Chat`, `Qwen/Qwen2.5-7B`, `Qwen/Qwen2.5-7B-Instruct`, etc. | âœ… |
| `Qwen2MoeForCausalLM` | Qwen2MoE | `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc. |  |
| `StableLmForCausalLM` | StableLM | `stabilityai/stablelm-3b-4e1t/`, `stabilityai/stablelm-base-alpha-7b-v2`, etc. |  |

## ä¾èµ–

- musa_toolkit >= dev3.0.0
- pytorch == v2.2.0
- [torch_musa](https://github.com/MooreThreads/torch_musa) >= v1.3.0
- triton >= v2.2.0
- ray >= 2.9
- vllm v0.4.2

## å®‰è£…
### ç›´æ¥å®‰è£…
ä»Releaseä¸­ä¸‹è½½ç¼–è¯‘å¥½çš„wheelæ–‡ä»¶ç›´æ¥å®‰è£…ã€‚å·²ç»åœ¨AutoDLæ‘©å°”çº¿ç¨‹ä¸“åŒºMTT S4000æœºå™¨ä¸ŠéªŒè¯è¿‡ï¼Œå…¶ä½™ç¯å¢ƒä¸ä¿è¯å¯ç”¨æ€§ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨æ‰‹åŠ¨ç¼–è¯‘çš„æ–¹å¼å®‰è£…ã€‚

```
pip install vllm-0.4.2+musa-cp310-cp310-linux_x86_64.whl
```
å¦‚æœå·²ç»å®‰è£…è¿‡æ²¡æ‰“è¡¥ä¸çš„vllmï¼Œå¯ä»¥ç”¨ä¸‹é¢çš„å‘½ä»¤è¦†ç›–å®‰è£…
```
pip install --force-reinstall --no-deps vllm-0.4.2+musa-cp310-cp310-linux_x86_64.whl
```
### æ‰‹åŠ¨ç¼–è¯‘
```
bash build_musa.sh
```
## æµ‹è¯•ç¤ºä¾‹
```
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer, LlamaForCausalLM
import transformers
import time
import torch
import torch_musa


model_path = <path_to_llm_model>

prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]

sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
llm = LLM(model=model_path, dtype="float16",trust_remote_code=True, device="musa")

outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

```

## Porting

å½“å‰ä»“åº“portingè‡ªvllm v0.4.2ç‰ˆæœ¬ã€‚å¦‚æœç”¨æˆ·å¸Œæœ›ä½¿ç”¨æ›´é«˜ç‰ˆæœ¬çš„vllmï¼Œåªéœ€è¦è¿è¡Œ`musa_porting.py`å°†åŸç”ŸCUDAä»£ç é€‚é…åˆ°MUSAä»£ç å³å¯ã€‚å½“ç„¶éšç€vllmçš„è¿­ä»£å¯èƒ½ä¼šæœ‰äº›ä»£ç æˆä¸ºæ¼ç½‘ä¹‹é±¼ï¼Œæ²¡æœ‰portingæˆåŠŸï¼Œç”¨æˆ·å¯è‡ªè¡Œä¿®æ”¹`musa_porting.py`æ–‡ä»¶ä¸­çš„æ–‡æœ¬æ›¿æ¢è§„åˆ™ã€‚ä»è€Œå‘æŒ¥MUSAå¼ºå¤§çš„CUDAå…¼å®¹èƒ½åŠ›ã€‚

### æ­¥éª¤
1. è¿è¡Œ `python musa_porting.py`
2. å°†`CMakeLists.txt`ä¸­éœ€è¦ç¼–è¯‘çš„æ–‡ä»¶åç¼€ä»`.cu`ä¿®æ”¹ä¸º`.mu`
3. ç¼–è¯‘è¿è¡Œvllm_musa

## è´¡çŒ®

æ¬¢è¿å¹¿å¤§ç”¨æˆ·åŠå¼€å‘è€…ä½¿ç”¨ã€åé¦ˆï¼ŒåŠ©åŠ›vllm_musaåŠŸèƒ½åŠæ€§èƒ½æŒç»­å®Œå–„ã€‚

ç¤¾åŒºå…±å»ºï¼ŒæœŸå¾…å¹¿å¤§å¼€å‘è€…ä¸æˆ‘ä»¬ä¸€é“ï¼Œå…±åŒæ‰“é€ MUSAè½¯ä»¶ç”Ÿæ€ã€‚æˆ‘ä»¬å°†é™†ç»­æ¨å‡ºä¸€ç³»åˆ—å¼€æºè½¯ä»¶MUSAåŠ é€Ÿé¡¹ç›®ã€‚

<details>
  <summary>VLLMåŸå§‹README</summary>
<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png">
    <img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png" width=55%>
  </picture>
</p>

<h3 align="center">
Easy, fast, and cheap LLM serving for everyone
</h3>

<p align="center">
| <a href="https://docs.vllm.ai"><b>Documentation</b></a> | <a href="https://vllm.ai"><b>Blog</b></a> | <a href="https://arxiv.org/abs/2309.06180"><b>Paper</b></a> | <a href="https://discord.gg/jz7wjKhh6g"><b>Discord</b></a> |

</p>

*Latest News* ğŸ”¥
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) in SF! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2024/01] Added ROCm 6.0 support to vLLM.
- [2023/12] Added ROCm 5.7 support to vLLM.
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) in SF! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/09] We created our [Discord server](https://discord.gg/jz7wjKhh6g)! Join us to discuss vLLM and LLM serving! We will also post the latest announcements and updates there.
- [2023/09] We released our [PagedAttention paper](https://arxiv.org/abs/2309.06180) on arXiv!
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/07] Added support for LLaMA-2! You can run and serve 7B/13B/70B LLaMA-2s on vLLM with a single command!
- [2023/06] Serving vLLM On any Cloud with SkyPilot. Check out a 1-click [example](https://github.com/skypilot-org/skypilot/blob/master/llm/vllm) to start the vLLM demo, and the [blog post](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/) for the story behind vLLM development on the clouds.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

---
## About
vLLM is a fast and easy-to-use library for LLM inference and serving.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with **PagedAttention**
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantization: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [SqueezeLLM](https://arxiv.org/abs/2306.07629), FP8 KV Cache
- Optimized CUDA kernels

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs and AMD GPUs
- (Experimental) Prefix caching support
- (Experimental) Multi-lora support

vLLM seamlessly supports many Hugging Face models, including the following architectures:

- Aquila & Aquila2 (`BAAI/AquilaChat2-7B`, `BAAI/AquilaChat2-34B`, `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.)
- Baichuan & Baichuan2 (`baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.)
- BLOOM (`bigscience/bloom`, `bigscience/bloomz`, etc.)
- ChatGLM (`THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.)
- Command-R (`CohereForAI/c4ai-command-r-v01`, etc.)
- DBRX (`databricks/dbrx-base`, `databricks/dbrx-instruct` etc.)
- DeciLM (`Deci/DeciLM-7B`, `Deci/DeciLM-7B-instruct`, etc.)
- Falcon (`tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.)
- Gemma (`google/gemma-2b`, `google/gemma-7b`, etc.)
- GPT-2 (`gpt2`, `gpt2-xl`, etc.)
- GPT BigCode (`bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, etc.)
- GPT-J (`EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.)
- GPT-NeoX (`EleutherAI/gpt-neox-20b`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.)
- InternLM (`internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.)
- InternLM2 (`internlm/internlm2-7b`, `internlm/internlm2-chat-7b`, etc.)
- Jais (`core42/jais-13b`, `core42/jais-13b-chat`, `core42/jais-30b-v3`, `core42/jais-30b-chat-v3`, etc.)
- LLaMA, Llama 2, and Meta Llama 3 (`meta-llama/Meta-Llama-3-8B-Instruct`, `meta-llama/Meta-Llama-3-70B-Instruct`, `meta-llama/Llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`, `openlm-research/open_llama_13b`, etc.)
- MiniCPM (`openbmb/MiniCPM-2B-sft-bf16`, `openbmb/MiniCPM-2B-dpo-bf16`, etc.)
- Mistral (`mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.)
- Mixtral (`mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, `mistral-community/Mixtral-8x22B-v0.1`, etc.)
- MPT (`mosaicml/mpt-7b`, `mosaicml/mpt-30b`, etc.)
- OLMo (`allenai/OLMo-1B-hf`, `allenai/OLMo-7B-hf`, etc.)
- OPT (`facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.)
- Orion (`OrionStarAI/Orion-14B-Base`, `OrionStarAI/Orion-14B-Chat`, etc.)
- Phi (`microsoft/phi-1_5`, `microsoft/phi-2`, etc.)
- Phi-3 (`microsoft/Phi-3-mini-4k-instruct`, `microsoft/Phi-3-mini-128k-instruct`, etc.)
- Qwen (`Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.)
- Qwen2 (`Qwen/Qwen1.5-7B`, `Qwen/Qwen1.5-7B-Chat`, etc.)
- Qwen2MoE (`Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.)
- StableLM(`stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.)
- Starcoder2(`bigcode/starcoder2-3b`, `bigcode/starcoder2-7b`, `bigcode/starcoder2-15b`, etc.)
- Xverse (`xverse/XVERSE-7B-Chat`, `xverse/XVERSE-13B-Chat`, `xverse/XVERSE-65B-Chat`, etc.)
- Yi (`01-ai/Yi-6B`, `01-ai/Yi-34B`, etc.)

Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source):

```bash
pip install vllm
```

## Getting Started

Visit our [documentation](https://vllm.readthedocs.io/en/latest/) to get started.
- [Installation](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)
- [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)
- [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):
```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

</details>